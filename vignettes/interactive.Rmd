---
title: "autoparallel-interactive"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel-interactive}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Interactive

This vignette discusses the interactive use of this package.

Suppose you have some large or unwieldy object in R which can conceptually
be split into parts, such as the rows of a data frame or the elements of a
list. You are analyzing this object, perhaps writing functions and
debugging them as you go. The code takes too long to run. Most of what
you're doing could happen in parallel after an appropriate split, but you
don't want to worry about all the issues that can arise with parallel code
when you're in the middle of your interactive analysis. You want a system
to manage all the details of the parallelism for you, so that you can focus
on the analysis. The `parallel_evaluator` (subsequently referred to as the
_'evaluator'_) is a system to manage the details.

## Basic Example

We'll start with a basic example.

```{r}

library(autoparallel)

x = list(1:10, rnorm(10), rep(pi, 10))

do = parallelize(x)
do

```

Calling `parallelize` split `x` into approximately equal parts
so that one can run code in parallel using the resulting evaluator `do`.

Calling `lapply` on a list through `do` produces the same result as the
base R case:

```{r}

lapply(x, head)

do(lapply(x, head))

```

There's nothing special about the use of `lapply()` above. We can evaluate
arbitrary code.

```{R}

y = 20
z = 30
do(y + z, verbose = TRUE)

```

This is designed for interactively building functions and analysis on large
data sets, or data sets that take too long to run. The interactive feature
is sending variables, including functions, from the manager's global
workspace to the parallel workers every time they are used. This allows us
to see the results of the improved / debugged version of the function as we
work on them.

```{r}

# An analysis function
myfun = function(x) x[1:2]

do(lapply(x, myfun))

# Oops I actually need the first 4
myfun = function(x) x[1:4]

# Now we see the new results of myfun
do(lapply(x, myfun))

```

## Limitations

`autoparallel` is not currently designed to work with multiple large
objects.  Rather, it was designed for a single large object to be
distributed to the workers when the evaluator is created.  The following
code will be slow because it serializes a large (400 MB) object to each of
the workers:

```{r, eval = FALSE}

# Any large R object
big = 1:1e8

object.size(big)

# BAD IDEA: this sends `big` over every time
do(sum(big + x[[1]][1]))

```

## Details

Under the hood, the evaluator is a closure with a couple attributes. The
most notable attributes are the variable name and the cluster. We can
inspect all this by printing the evaluator as a function.

Under the hood `do` sent the code to different R processes for
evaluation. 

```{R}

print.function(do)

```

The default simplifying function is `c()`. We can also bring back results
without simplifying.

```{R}

do(lapply(x, head), simplify = FALSE)

```


## Cleaning up

When finished it's a good idea to shut down the cluster.
The cluster is an attribute on the evaluator.

```{r}

parallel::stopCluster(attr(do, "cluster"))

```

## Working with many files

A realistic example is working with many files simultaneously. The US
Veterans Administration (VA) Court appeals are one such example. Each file
contains the summary of an appeal.
One can download a handful from the VA servers as follows:

```{r, echo = FALSE}

datadir = "~/data/vets/vets_appeals"

```

```{r download, eval = FALSE}

datadir = "vets_appeals"
dir.create(datadir)

fnames = paste0("1719", 100:266, ".txt")
urls = paste0("https://www.va.gov/vetapp17/files3/", fnames)

Map(download.file, urls, fnames)

```

The file names themselves are small, so we can cheaply distribute them
among the parallel workers.

```{r}

filenames = list.files(datadir, full.names = TRUE)
length(filenames)

do = parallelize(filenames)

```

The following code actually loads the data contained in the files and
assigns the result into `appeals` on the cluster. 

```{r}

do({
    appeals <- lapply(filenames, readLines)
    appeals <- sapply(appeals, paste, collapse = "\n")
    appeals <- enc2utf8(appeals)
    NULL
})

```

The braces along with the final `NULL` are necessary to avoid transferring
the large data set from the workers back to the manager.

The code above only assigned `appeals` to the global environment of the
workers. It does not exist in the manager process.

```{r}

"appeals" %in% ls()

```

However, if we subsequently create a variable called `appeals` in the
manager process then the evaluator will export it to the cluster,
overriding the existing one.  

```{R}

ten = 10
do(ten + 1, verbose = TRUE)

```

The evaluator allows us to execute the same code that is
used for serial R. In this case 

```{r}

do(length(appeals))
do(class(appeals))

```

We may want to look more closely at those cases which have been remanded
for further evidence. If they're a reasonably small subset we may choose to
bring them back into the manager process for further non parallel analysis.
This would be useful to see the warnings that may come from our code, for example.

```{r}

# Check how many we're about to bring back
do(sum(grepl("REMAND", appeals)))

# Bring them back from the workers
remand <- do(appeals[grepl("REMAND", appeals)])

length(remand)

```

In summary, when working with larger data sets it's efficient to minimize
the data movement. We avoided it in this case by only distributing the
relatively small vector of file names and having each worker independently
load the files that it needed, thus keeping the data in place on that
worker.

```{R}

parallel::stopCluster(attr(do, "cluster"))

```
